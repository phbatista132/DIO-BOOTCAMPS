{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phbatista132/DIO-BOOTCAMPS/blob/main/BairesDev%20ML/YOLO/redeyolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll6XWs4WeErH"
      },
      "source": [
        "![Notebook Title](http://blog.ibanyez.info/download/B20190408T000000071.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqo1gtPX6BXO"
      },
      "source": [
        "# How to train YOLOv3 using Darknet on Colab notebook and optimize the VM runtime load times\n",
        "\n",
        "## Welcome!\n",
        "\n",
        "This Colab notebook will show you how to:\n",
        "\n",
        "* Train a **Yolo v3** model using **Darknet** using the Colab **12GB-RAM GPU**.\n",
        "* Turn Colab notebooks into an effective tool to work on real projects. Dealing with the handicap of a runtime that will **blow up every 12 hours** into the space!\n",
        "  * Working directly from the files on your computer.\n",
        "  * Configure your notebook to install everything you need and start training in about a minute (Tested using 550MB dataset).\n",
        "  * Receive your trained weights directly on your computer during the training. While the notebook is training you can check how it is going using your trained weights in your computer.\n",
        "\n",
        "\n",
        "#### This notebook is part of the post [How to train YOLOv3 using Darknet framework and optimize the VM runtime load times](http://blog.ibanyez.info/blogs/coding/20190410-run-a-google-colab-notebook-to-train-yolov3-using-darknet-in/) I encourage you to visit! You will find a deeper explanation about Google Colab, the goods and the limitations of this great tool.\n",
        "\n",
        "\n",
        "### These are the steps we'll follow:\n",
        "\n",
        "* Configure Google Drive and map as network  _Drive_.\n",
        "* Some utils to help to do some tasks.\n",
        "* Configure the pre-requisites on the runtime.\n",
        "  * Check the CUDA installation on the runtime VM.\n",
        "  * Install cuDNN.\n",
        "  * Clone and compile Darknet. We'll use a repo based on [AlexeyAB's Darknet repo](https://github.com/AlexeyAB/darknet/). I applied  some changes to make possible to load files from `/My Drive/` and reduced the number of logs on console to speed up the notebook.\n",
        "  * We'll check that everything works great.\n",
        "* Explained how to manage your YOLO files on your computer and it will be used transparently from this notebook.\n",
        "\n",
        "> _**NOTE:** Cells with an annotation **`# Not Necessary cell`**. Can be removed without having any impact. They are only explanatory content._\n",
        "\n",
        "### Without further ado, let's start!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4J9sFi39ScM"
      },
      "source": [
        "## STEP 0. Configure runtime to work with GPU\n",
        "\n",
        "We want to use the **12GB-RAM GPU** hardware acceleration!\n",
        "\n",
        "Go to **> Menu > Runtime > Configure Runtime Type** And select **GPU** From the **Hardware accelerator** drop down meu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOJg1x9gLvcj"
      },
      "source": [
        "## STEP 1. Connect your files to Google Drive\n",
        "In order to have your files in your local computer you need to install and configure Google Backup and Sync to keep one folder of your Drive synced with a folder on your computer.\n",
        "\n",
        "![schema drive.jpg](http://blog.ibanyez.info/download/B20190408T000000060.jpg)\n",
        "\n",
        "1. Create a folder on your Google Drive named _**darknet**_\n",
        "2. Configure the Google Backup and Sync as follows. If you don't speak Spanish, you maybe learn some words! **;)**\n",
        "![Sync Drive.jpg](http://blog.ibanyez.info/download/B20190408T000000063.jpg)\n",
        "\n",
        "**After this step you'll have a folder called _darknet_ in your local computer. This folder is where you will work with files on your computer**\n",
        "\n",
        "> _**TIP** - We need to have a good performance downloading data from Drive to Colab. Having a lot of files in your Drive root folder can slow down the things quite a bit. It's a good practice working with Colab to move all your root folder files into a folder_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaAwkZv0rBQd"
      },
      "source": [
        "## STEP 2. Connect the Colab notebook to Google Drive\n",
        "\n",
        "Now we're gonna map your Google Drive folder. This first step is the only one that will require your manual interaction every time you run your notebook.\n",
        "\n",
        "* Execute the following cell _(Click on Play button or press CTRL + ENTER)_ and click on the link to authorize your notebook to access to your Google Drive.\n",
        "* Paste the code Google will give to you and push `enter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJZRcEw0LoBd"
      },
      "source": [
        "# This cell imports the drive library and mounts your Google Drive as a VM local drive. You can access to your Drive files\n",
        "# using this path \"/content/gdrive/My Drive/\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8pPjCXXvASq"
      },
      "source": [
        "Congratulations! Now you can access to your local computer folder directly from here!\n",
        "\n",
        "Check it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrdyBxyZvLpM"
      },
      "source": [
        "# Not Necessary cell\n",
        "# List the content of your local computer folder\n",
        "!ls -la \"/content/gdrive/My Drive/darknet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGNcZ9EtOK7k"
      },
      "source": [
        "!sudo apt-get install tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGsRuXM-OQBb"
      },
      "source": [
        "!tree /content/gdrive/My\\ Drive/darknet/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1_APkL1nYbz"
      },
      "source": [
        "### UPDATE NOTE\n",
        "\n",
        "If you want to simplify your paths, you can use a Symbolic link:\n",
        "\n",
        "`!ln -s \"/content/gdrive/My Drive/darknet/\" /mydrive`\n",
        "\n",
        " Then you'll be able to access your Google Drive files just using `/mydrive` path\n",
        "\n",
        "**BE CAREFUL: All the paths in this notebook are without using Symbolic link. You'll have to remember to change the path everywhere**\n",
        "\n",
        "Thanks to **Dennis Kashkin** for this recommendation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yreDPcJdoo29"
      },
      "source": [
        "# Uncomment if you want to use Symbolic link\n",
        "#!ln -s /content/gdrive/My\\ Drive/darknet/ /mydrive\n",
        "#!ls /mydrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db68v7TmMTmR"
      },
      "source": [
        "## STEP 2. Check CUDA release version\n",
        "\n",
        "Nvidia CUDA is pre-installed on Colab notebooks. Now we'll check the version installed.\n",
        "\n",
        "> _**BE AWARE:** Maybe some time from the time I'm writing these lines (April 9th, 2019)  the CUDA version is upgraded on Colab and you should download another version of the cuDNN in the next step. Now is release 10.0 and we are using cuDNN (cudnn-10.0-linux-x64-v7.5.0.56.tgz) accordingly_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew5eGbl9MdKL"
      },
      "source": [
        "# This cell can be commented once you checked the current CUDA version\n",
        "# CUDA: Let's check that Nvidia CUDA is already pre-installed and which version is it. In some time from now maybe you\n",
        "!/usr/local/cuda/bin/nvcc --version\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nkYzWwMuBk"
      },
      "source": [
        "## STEP 3. Install cuDNN according to the current CUDA version\n",
        "Now, you need to download cuDNN from Nvidia web site. You'll need to sign up on the site.\n",
        "\n",
        "* Download cuDNN from [Nvidia website](https://developer.nvidia.com/cudnn)\n",
        "\n",
        "  * Right now, because we have _**CUDA 10.0**_ preinstalled in Colab runtime, you need download [cuDNN v7.5.0.56 for CUDA v10.0](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.5.0.56/prod/10.0_20190219/cudnn-10.0-linux-x64-v7.5.0.56.tgz) - the file is cudnn-10.0-linux-x64-v7.5.0.56.tgz\n",
        "\n",
        "* On your local computer, create a folder named _**cuDNN**_ in your local folder _**darknet**_. Copy the _**tgz**_ file there\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_A8LDvyM7x5"
      },
      "source": [
        "# We're unzipping the cuDNN files from your Drive folder directly to the VM CUDA folders\n",
        "!tar -xzvf gdrive/My\\ Drive/darknet/cuDNN/cudnn-10.0-linux-x64-v7.5.0.56.tgz -C /usr/local/\n",
        "!chmod a+r /usr/local/cuda/include/cudnn.h\n",
        "\n",
        "# Now we check the version we already installed. Can comment this line on future runs\n",
        "!cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9BFQOfNowN"
      },
      "source": [
        "## STEP 4. Installing Darknet\n",
        "Great!! We have all the necessary to start working with Darknet.\n",
        "\n",
        "This notebook works with a slightly modified version of darknet, which is based on the [AlexeyAB Darknet repo](https://github.com/AlexeyAB/darknet/)\n",
        "The changes applied to the original repo are:\n",
        "* Allowing to use spaces on the darknet configuration files _**obj.data**_. Necessary to work with Google Drive directly.\n",
        "* Removing some logs on every epoch. The original repo write more logs than the Colab notebook can sync. This creates a long queue during the training. This version only shows the results after every iteration.\n",
        "\n",
        "You can take a look to the code at the [github repo](https://github.com/kriyeng/darknet/)\n",
        "\n",
        "> _**TRICK**: Because we want to run the notebook fast every time we will compile darknet only the first time we run this notebook. Then, we'll save the compiled version to your drive. For the future executions we'll copy the compiled one instead of compiling again._\n",
        "\n",
        "\n",
        "**Here comes our first trick to speed up the runtime load time**\n",
        "\n",
        "* The first time we will:\n",
        "  * Clone and compile the darknet project.\n",
        "  * Copy the compiled version to our Google Drive Floder\n",
        "  \n",
        "* The next times, instead of compiling it again on every runtime load, we'll copy the compiled version to our VM machine!\n",
        "\n",
        "When compiling the first time, your output last line has to be something like this:\n",
        "\n",
        "`g++ -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv` -DGPU (...)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXi9S5XAIP3A"
      },
      "source": [
        "## STEP 4-A. Cloning and compiling Darkent. ONLY NEEDS TO BE RUN ON THE FIRST EXECUTION!!\n",
        "In this step we'll clone the darkent repo and compile it.\n",
        "* Clone Repo\n",
        "* Compile Darknet\n",
        "* Copy compiled version to Drive\n",
        "\n",
        "When compiling ends, your output last line has to be something like this:\n",
        "\n",
        "_`g++ -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv` -DGPU (...)_`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt0Y06QTNyZG"
      },
      "source": [
        "# Leave this code uncommented on the very first run of your notebook or if you ever need to recompile darknet again.\n",
        "# Comment this code on the future runs.\n",
        "!git clone https://github.com/kriyeng/darknet/\n",
        "%cd darknet\n",
        "\n",
        "# Check the folder\n",
        "!ls\n",
        "\n",
        "# I have a branch where I have done the changes commented above\n",
        "!git checkout feature/google-colab\n",
        "\n",
        "#Compile Darknet\n",
        "!make\n",
        "\n",
        "#Copies the Darknet compiled version to Google drive\n",
        "!cp ./darknet /content/gdrive/My\\ Drive/darknet/bin/darknet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrVEej6lIHTI"
      },
      "source": [
        "## STEP 4-B. Copying the compiled version of Darknet from Drive. UNCOMMENT AFTER FIRST EXECUTION\n",
        "Copy the darknet compiled version from drive to the VM.\n",
        "* Make the local darknet folder\n",
        "* Copy the darknet file\n",
        "* Set execution permissions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAN2TNZ007c_"
      },
      "source": [
        "# Uncomment after the first run, when you have a copy of compiled darkent in your Google Drive\n",
        "\n",
        "# Makes a dir for darknet and move there\n",
        "#!mkdir darknet\n",
        "#%cd darknet\n",
        "\n",
        "# Copy the Darkent compiled version to the VM local drive\n",
        "#!cp /content/gdrive/My\\ Drive/darknet/bin/darknet ./darknet\n",
        "\n",
        "# Set execution permissions to Darknet\n",
        "#!chmod +x ./darknet\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH7QRxBUOPiz"
      },
      "source": [
        "## STEP 5. Runtime configuration finished!\n",
        "Let's chek it out!\n",
        "\n",
        "If you are running this notebook for the first time, you can run the following cells in order to check if everything goes as expected!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD378FjcrurC"
      },
      "source": [
        "## Some Utils\n",
        "Let's add some utils that maybe can be useful.\n",
        "\n",
        "These utils are:\n",
        "* imgShow() - Will help us to show an image in the remote VM\n",
        "* download() - Will allow you to get some file from your notebook in case you need to\n",
        "* upload() - You can upload files to your current folder on the remote VM.\n",
        "\n",
        "Thanks to [Ivan Goncharov](https://twitter.com/Ivangrov) for these helpers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD5FBWmjrsks"
      },
      "source": [
        "#download files\n",
        "def imShow(path):\n",
        "  import cv2\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  image = cv2.imread(path)\n",
        "  height, width = image.shape[:2]\n",
        "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18, 10)\n",
        "  plt.axis(\"off\")\n",
        "  #plt.rcParams['figure.figsize'] = [10, 5]\n",
        "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def upload():\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  for name, data in uploaded.items():\n",
        "    with open(name, 'wb') as f:\n",
        "      f.write(data)\n",
        "      print ('saved file', name)\n",
        "def download(path):\n",
        "  from google.colab import files\n",
        "  files.download(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCou8imNOTTN"
      },
      "source": [
        "# Not necessary cell\n",
        "# Get yolov3 weights\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvySKrnh2cvG"
      },
      "source": [
        "**NOTE:** The following test only will work when the darknet is compiled in the runtime. This demo uses some data from the original darknet folders. For your Object detection projects, you'll have these necessary files on your local folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op1iEE3bOVR6"
      },
      "source": [
        "# Not necessary cell\n",
        "# Execute darknet using YOLOv3 model with pre-trained weights to detect objects on 'person.jpg'\n",
        "!./darknet detect cfg/yolov3.cfg yolov3.weights data/person.jpg -dont-show\n",
        "\n",
        "# Show the result using the helper imgShow()\n",
        "imShow('predictions.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZreQ_407H1uw"
      },
      "source": [
        "## If you can see the same picture as the one below, congratulations!! At this point you have Darknet configured and working!\n",
        "\n",
        "![person.jpg](http://blog.ibanyez.info/download/B20190409T000000064.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0t221djS1Gk"
      },
      "source": [
        "# PART 2. Training YOLO\n",
        "\n",
        " > _**TRICK:** Every time you want to run all your cells automatically you can go to the **> Menu > Runtime > run all**. Maybe you don't want to execute the entire notebook. You can write the following cell where you want to stop the process and uncoment the **`assert False`** command. This will throw an error and will stop to run more cells. Thanks to: [This thread](https://groups.google.com/forum/#!topic/jupyter/ELftSFSiedQ)_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQjODqIAS-er"
      },
      "source": [
        "# don't go beyond here with Run All\n",
        "#assert False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzMqLZV-rF5"
      },
      "source": [
        "## PART 2. STEP 0. Preparing your data and configuration files\n",
        "\n",
        "Before going further let's take a look at what configuration files you need to have in your local drive _`darknet`_\n",
        "\n",
        "![Yolov3 configuration files cheat sheet.jpg](http://blog.ibanyez.info/download/B20190410T000000072.png)\n",
        "\n",
        "You can download the cheat sheet [here](http://blog.ibanyez.info/download/B20190410T000000072.png)\n",
        "\n",
        "If you need deeper explanations on how to prepare your data sets, annotation and deep learning, visit [How to train YOLOv3 using Darknet framework and optimize the VM runtime load times](post link)\n",
        "\n",
        "\n",
        "> **TRICK:** You have to be carefully configuring paths on your config files. _*obj.data*_ file needs to have spaces on the path escaped with _**\\**_. Like this: **_/content/gdrive/My\\ Drive/darknet/train.txt_**. But, in files **_train.txt_** and **_test.txt_** does not!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKl1l_NgBn6y"
      },
      "source": [
        "## PART 2. STEP 1. Loading files to VM local drive\n",
        "The network speed between Google Drive and Colab VM can have an impact on your training speed accessing to your dataset images.\n",
        "\n",
        "You can have 3 possible approaches depending on the size of your dataset.\n",
        "\n",
        "> _**NOTE:** This step is not necessary for all the configuration files and weights. They can be accessed directly from Google Drive without considerable performance issues. **This step is only for the dataset images and annotations**_\n",
        "\n",
        "* **Option 1** - You can try  to use directly the files from Google Drive _`img/`_ folder. Depending on your dataset maybe this can be more than good.\n",
        "* **Option 2** - Before start training copy your dataset from Google Drive to the local VM filesystem. Maybe can be a good practice to copy as one single tar file and decompress in your VM local _`img/`_ folder\n",
        "* **Option 3** - If your dataset is quite big, maybe you can upload to a git repository and clone from here. Usually transfer time between are much better. If you have to decide I have the feeling that bitbucket have better speed transfer times than github, but please, don't take this as confirmed, **I haven't done specific tests on that, I could be wrong!**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cp5EsZOEBv8"
      },
      "source": [
        "#### PART 2. STEP 1 - Option 1. Using files from Google Drive directly.\n",
        "You don't have to do anything here. Your **_train.txt_** should have the correct path:\n",
        "* **/content/grdive/My Drive/darknet/img/image001.jpg**. As said before, don't use escaped white space for the paths on _**train.txt**_ and _**test.txt**_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6zm1GHDF-8_"
      },
      "source": [
        "#### PART 2. STEP 1 - Option 2A. Copying files from Google Drive to VM local filesystem.\n",
        "Execute the follow cell to copy your files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WewV3jU3B4Eo"
      },
      "source": [
        "# Copy fils from Google Drive to the VM local filesystem\n",
        "!cp -r \"/content/gdrive/My Drive/darknet/img\" ./img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhUzh9F-GehW"
      },
      "source": [
        "#### PART 2. STEP 1 - Option 2B. Copying files zipped from Google Drive to VM local filesystem and unzip locally.\n",
        "Execute the follow cell to copy your files and uncompress.\n",
        "You can use _*!ls*_ command to esnure what's the correct path you have to configure in your _*train.txt*_ to correctly access to your dataset images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjnV7SDOGpdN"
      },
      "source": [
        "# Copy your compressed file\n",
        "#!cp -r \"/content/gdrive/My Drive/darknet/img/img.tgz\" ./img\n",
        "\n",
        "# Uncompress zipped file\n",
        "#!tar -xzvf ./img/img.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trq0Nnt9Hdns"
      },
      "source": [
        "#### PART 2. STEP 1 - Option 3. Clone your image dataset from a git repo. Seems the fastest one.\n",
        "Execute the follow cell to clone your dataset repo to VM local filesystem\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pa1Z9cxC290"
      },
      "source": [
        "# Git clone directly to ./img folder\n",
        "#!git clone https://[your-repository] ./img\n",
        "\n",
        "# Check the result - Uncomment when you checked for speed up further runs\n",
        "#!ls -la ./img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_4747QHIGZ7"
      },
      "source": [
        "## PART 3. Finally, Train your model!\n",
        "\n",
        "When you execute the following command, your model will start training.\n",
        "\n",
        "You will have a log line per epoch. On each iteration you will see how your training is going.\n",
        "\n",
        "> **TRICK: Darknet copies a backup of your trained weights every 100 iterations. As magic, this file will be synced to your local drive on your computer, as well as the backups darknet do every 1000 iterations, saving it on a separate file.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870c953b"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f25915ec"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48259253"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf876ee"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c516ec30"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c975b5aa"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63b645fe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304078b6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47e1834"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae185bca"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90a8f192"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "206edf7e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7b73cbd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d11b2a44"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "367356bb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8b1d6af"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2b6897"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0b38d3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5b30fe5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68283932"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a278f9b2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e922ca9e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c7462d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cb6e9d2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8477cee5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b69d7812"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5206b9b8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dea777c7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd9d920e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904aed9a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe94ce24"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26a0e957"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3b00635"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "986ed49d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e3df045"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52f1606"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b1e9d6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "087acade"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ac84ed4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49723d27"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1f5487"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86229366"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f153fb0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee2b6a9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8580c1c2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227802d4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b866e8f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06163d3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2e4b6e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "829c47e4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00134c01"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7da8db18"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7304306f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1460a9d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baedd246"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8277ad"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b5e0893"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acd30f22"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d6664e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb070511"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13fRzkiQiPhW"
      },
      "source": [
        "!./darknet detector train \"/content/gdrive/My Drive/darknet/obj.data\" \"/content/gdrive/My Drive/darknet/yolov3.cfg\" \"/content/gdrive/My Drive/darknet/darknet53.conv.74\" -dont_show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oWYkXA0LtCQ"
      },
      "source": [
        "## PERFORMANCE TIPS & TRICKS\n",
        "\n",
        "* **Speed up load times of the runtime:** When everything is checked that works, you can remove cells or comment unnecessary lines of code to make your loading time lower on every run.\n",
        "\n",
        "* **How to keep your notebook alive for more time?:** Keep you browser with your notebook open. If you close your browser, your notebook will reach the iddle time and will be removed from Colab cloud Service. (90 minutes)\n",
        "  \n",
        "* **Re-run your training after reaching the limitation time for Colab runtimes (12 hours):**\n",
        "  * Open a new notebook or reconnect the current one.\n",
        "  * Comment the cell above and uncomment the cell below.\n",
        "  * In your local computer, copy the file **backup/yolov3_last.weights** to your local computer **weights/** folder.\n",
        "  * Execute Run all in the **> menu > Runtime > Run All**\n",
        "  * _The copy step is not absolutely necessary, but I like to keep a copy of the last training session and not overwrite this file on next trainings._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "josdFFdVMyj3"
      },
      "source": [
        "# Start training at the point where the last runtime finished\n",
        "#!./darknet detector train \"/content/gdrive/My Drive/darknet/obj.data\" \"/content/gdrive/My Drive/darknet/yolov3.cfg\" \"/content/gdrive/My Drive/darknet/weights/yolov_last.weights\" -dont_show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3C8fIrVKR4T"
      },
      "source": [
        "## TROUBLESHOOTING\n",
        "The main problems you can face if your model throw an error is:\n",
        "\n",
        "* Images or files not found. Check the **Yolov3 cheat sheet** image above and check that everything is ok.\n",
        "* If have you configured wrongly your **filters** and **classes** in the **yolov3.cfg**. Check the **Yolov3 cheet sheet** above.\n",
        "* You can face some out of memory or library errors mainly for the lack of some the pre-requisits. In this case, check the versions of the current libraries installed on your Colab VM. You can find more information in the first steps of this notebook.\n",
        "* **Batch** and **subdivisions** parameters on your **yolov3.cfg** can affect to the memory allocation as well. Refer to the original repo [Here]() for further details.\n",
        "\n",
        "### TROUBLESHOOTING UPDATE\n",
        "Be careful if you are preparing your files on Windows. If you use **CRLF** on your files instead of **LF** You can have problems opening the files correctly. - Thanks to [Satya Shetty](https://twitter.com/satyashetty) for sharing this issue!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46LRTt-5Pr52"
      },
      "source": [
        "## About me\n",
        "\n",
        "You can find the original post with more explanations about this notebook at [How to train YOLOv3 using Darknet framework and optimize the VM runtime load times](post link)\n",
        "\n",
        "I'm David Ibañez from Barcelona. Feel free to get in touch!\n",
        "\n",
        "* You can visit my blog at [Dev-ibanyez.info](http://blog.ibanyex.info)\n",
        "* You can get in touch with me on [Twitter](https://twitter.com/dav_ibanez)\n",
        "* You can get in touch or contribute to this notebook at [Github](https://github.com/kriyeng/yolo-on-colab-notebook/)\n",
        "* You can comment on the [dev.to post about this notebook ](PENDING)\n",
        "\n",
        "Thanks for you having read this notebook! :clap: :clap: :clap:\n",
        "\n",
        "## SOURCES\n",
        "\n",
        "\n",
        "#### Other sources\n",
        "* YOLO original web site [Joseph Redmon Page](https://pjreddie.com/darknet/yolo/)\n",
        "* AlexeyAB darknet repo [github](https://github.com/AlexeyAB/darknet/)\n",
        "* The Ivan Goncharov [notebook](https://github.com/ivangrov/YOLOv3-GoogleColab/blob/master/YOLOv3_GoogleColab.ipynb) inspired me to try Google Colab and end up creating this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f63f579"
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ac57839"
      },
      "source": [
        "!unzip train2014.zip\n",
        "!unzip val2014.zip\n",
        "!unzip annotations_trainval2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dda16531"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/data/airline_passenger_satisfaction.csv')\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Qjf2wxi9xI"
      },
      "source": [
        "df = pd.read_csv('/home/jupyter/data/airline_passenger_satisfaction.csv')\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yInDLYkWjAVM"
      },
      "source": [
        "df = pd.read_csv('/kaggle/input/airline-passenger-satisfaction/airline_passenger_satisfaction.csv')\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e74faa53"
      },
      "source": [
        "!pip install pycocotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3bdedd9"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the desired classes (including original YOLO classes to keep)\n",
        "desired_classes = ['cat', 'dog', 'person', 'car', 'bicycle', 'bus']\n",
        "\n",
        "# Path to the COCO annotations file\n",
        "annotation_file = 'annotations/instances_train2014.json'\n",
        "\n",
        "# Load the COCO annotations\n",
        "coco = COCO(annotation_file)\n",
        "\n",
        "# Get all class IDs and class names\n",
        "categories = coco.loadCats(coco.getCatIds())\n",
        "category_names = [cat['name'] for cat in categories]\n",
        "\n",
        "# Get the IDs of the desired classes\n",
        "desired_class_ids = coco.getCatIds(catNms=desired_classes)\n",
        "\n",
        "# Filter annotations to keep only those with desired class IDs\n",
        "filtered_annotations = []\n",
        "image_ids_to_keep = set()\n",
        "\n",
        "for img_id in coco.getImgIds():\n",
        "    ann_ids = coco.getAnnIds(imgIds=img_id, catIds=desired_class_ids, iscrowd=None)\n",
        "    if ann_ids:\n",
        "        filtered_annotations.extend(coco.loadAnns(ann_ids))\n",
        "        image_ids_to_keep.add(img_id)\n",
        "\n",
        "# Create directories for filtered images and annotations\n",
        "filtered_images_dir = 'coco_filtered/images'\n",
        "filtered_annotations_dir = 'coco_filtered/annotations'\n",
        "os.makedirs(filtered_images_dir, exist_ok=True)\n",
        "os.makedirs(filtered_annotations_dir, exist_ok=True)\n",
        "\n",
        "# Copy filtered images to the new directory\n",
        "for img_id in image_ids_to_keep:\n",
        "    img_info = coco.loadImgs(img_id)[0]\n",
        "    src_path = os.path.join('train2014', img_info['file_name'])\n",
        "    dest_path = os.path.join(filtered_images_dir, img_info['file_name'])\n",
        "    shutil.copyfile(src_path, dest_path)\n",
        "\n",
        "# Save the filtered annotations to a new JSON file (this is just the annotations, not the full coco format)\n",
        "import json\n",
        "with open(os.path.join(filtered_annotations_dir, 'filtered_instances_train2014.json'), 'w') as f:\n",
        "    json.dump(filtered_annotations, f)\n",
        "\n",
        "print(f\"Filtered annotations and images for {len(image_ids_to_keep)} images and {len(filtered_annotations)} annotations saved.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a869bc09"
      },
      "source": [
        "# Path to the COCO validation annotations file\n",
        "annotation_file_val = 'annotations/instances_val2014.json'\n",
        "\n",
        "# Load the COCO validation annotations\n",
        "coco_val = COCO(annotation_file_val)\n",
        "\n",
        "# Get the IDs of the desired classes (using the same list as training)\n",
        "desired_class_ids = coco_val.getCatIds(catNms=desired_classes)\n",
        "\n",
        "# Filter validation annotations to keep only those with desired class IDs\n",
        "filtered_annotations_val = []\n",
        "image_ids_to_keep_val = set()\n",
        "\n",
        "for img_id in coco_val.getImgIds():\n",
        "    ann_ids = coco_val.getAnnIds(imgIds=img_id, catIds=desired_class_ids, iscrowd=None)\n",
        "    if ann_ids:\n",
        "        filtered_annotations_val.extend(coco_val.loadAnns(ann_ids))\n",
        "        image_ids_to_keep_val.add(img_id)\n",
        "\n",
        "# Create directory for filtered validation images and annotations\n",
        "filtered_images_dir_val = 'coco_filtered/val_images'\n",
        "filtered_annotations_dir_val = 'coco_filtered/val_annotations'\n",
        "os.makedirs(filtered_images_dir_val, exist_ok=True)\n",
        "os.makedirs(filtered_annotations_dir_val, exist_ok=True)\n",
        "\n",
        "# Copy filtered validation images to the new directory\n",
        "for img_id in image_ids_to_keep_val:\n",
        "    img_info = coco_val.loadImgs(img_id)[0]\n",
        "    src_path = os.path.join('val2014', img_info['file_name'])\n",
        "    dest_path = os.path.join(filtered_images_dir_val, img_info['file_name'])\n",
        "    shutil.copyfile(src_path, dest_path)\n",
        "\n",
        "# Save the filtered validation annotations to a new JSON file\n",
        "with open(os.path.join(filtered_annotations_dir_val, 'filtered_instances_val2014.json'), 'w') as f:\n",
        "    json.dump(filtered_annotations_val, f)\n",
        "\n",
        "print(f\"Filtered validation annotations and images for {len(image_ids_to_keep_val)} images and {len(filtered_annotations_val)} annotations saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a5e3e8"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19903fb5"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def coco_to_yolo(coco_annotations_path, output_dir, category_map):\n",
        "    \"\"\"Converts COCO annotations to YOLO format.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(coco_annotations_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create a dictionary to map image_id to image_info\n",
        "    images = {img['id']: img for img in data}\n",
        "\n",
        "    # Group annotations by image_id\n",
        "    annotations_by_image = {}\n",
        "    for ann in data:\n",
        "        image_id = ann['image_id']\n",
        "        if image_id not in annotations_by_image:\n",
        "            annotations_by_image[image_id] = []\n",
        "        annotations_by_image[image_id].append(ann)\n",
        "\n",
        "\n",
        "    for image_id, annotations in tqdm(annotations_by_image.items(), desc=\"Converting annotations\"):\n",
        "        img_info = images[image_id]\n",
        "        img_width = img_info['width']\n",
        "        img_height = img_info['height']\n",
        "        file_name = img_info['file_name']\n",
        "        output_path = os.path.join(output_dir, os.path.splitext(file_name)[0] + '.txt')\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            for ann in annotations:\n",
        "                category_id = ann['category_id']\n",
        "                if category_id in category_map:\n",
        "                    yolo_class_id = category_map[category_id]\n",
        "                    bbox = ann['bbox']\n",
        "                    # Convert [x, y, width, height] to [center_x, center_y, width, height] (normalized)\n",
        "                    x_center = (bbox[0] + bbox[2] / 2) / img_width\n",
        "                    y_center = (bbox[1] + bbox[3] / 2) / img_height\n",
        "                    bbox_width = bbox[2] / img_width\n",
        "                    bbox_height = bbox[3] / img_height\n",
        "\n",
        "                    f.write(f\"{yolo_class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
        "\n",
        "# Define the mapping from COCO category IDs to YOLO class IDs\n",
        "# You need to adjust this mapping based on the desired_classes defined in the previous subtask\n",
        "# and the order in which you want your YOLO classes to be indexed (starting from 0).\n",
        "# Example: {'coco_cat_id_person': 0, 'coco_cat_id_cat': 1, ...}\n",
        "# To get the COCO category IDs, you would typically load the original COCO annotations JSON\n",
        "# and find the IDs for your desired class names.\n",
        "\n",
        "# Assuming you have access to the original coco object from the previous subtask\n",
        "# If not, you would need to load it again or find a way to get the mapping.\n",
        "# For demonstration purposes, let's create a dummy mapping based on the desired_classes\n",
        "# and assume a simple integer mapping starting from 0.\n",
        "# In a real scenario, you would get the actual COCO category IDs.\n",
        "\n",
        "# Let's reload the original COCO annotations to get the category IDs\n",
        "annotation_file_train = 'annotations/instances_train2014.json'\n",
        "coco_train = COCO(annotation_file_train)\n",
        "categories = coco_train.loadCats(coco_train.getCatIds())\n",
        "category_name_to_id = {cat['name']: cat['id'] for cat in categories}\n",
        "\n",
        "desired_classes = ['cat', 'dog', 'person', 'car', 'bicycle', 'bus']\n",
        "category_map = {}\n",
        "for i, class_name in enumerate(desired_classes):\n",
        "    if class_name in category_name_to_id:\n",
        "        category_map[category_name_to_id[class_name]] = i\n",
        "\n",
        "print(\"Category mapping (COCO ID to YOLO ID):\", category_map)\n",
        "\n",
        "# Convert filtered training annotations\n",
        "coco_to_yolo('coco_filtered/annotations/filtered_instances_train2014.json', 'coco_filtered/labels', category_map)\n",
        "\n",
        "# Convert filtered validation annotations\n",
        "coco_to_yolo('coco_filtered/val_annotations/filtered_instances_val2014.json', 'coco_filtered/val_labels', category_map)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMPemCpLk1tZ"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO # Import COCO again to load image info\n",
        "\n",
        "def coco_to_yolo_fixed(coco_annotations_path, original_coco_images_path, output_dir, category_map):\n",
        "    \"\"\"Converts COCO annotations to YOLO format, robust to filtered annotations.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(coco_annotations_path, 'r') as f:\n",
        "        filtered_annotations = json.load(f)\n",
        "\n",
        "    # Load original COCO image information\n",
        "    with open(original_coco_images_path, 'r') as f:\n",
        "        original_coco_data = json.load(f)\n",
        "        images = {img['id']: img for img in original_coco_data['images']}\n",
        "\n",
        "    # Group filtered annotations by image_id\n",
        "    annotations_by_image = {}\n",
        "    for ann in filtered_annotations:\n",
        "        image_id = ann['image_id']\n",
        "        if image_id not in annotations_by_image:\n",
        "            annotations_by_image[image_id] = []\n",
        "        annotations_by_image[image_id].append(ann)\n",
        "\n",
        "    for image_id, annotations in tqdm(annotations_by_image.items(), desc=\"Converting annotations\"):\n",
        "        if image_id in images:\n",
        "            img_info = images[image_id]\n",
        "            img_width = img_info['width']\n",
        "            img_height = img_info['height']\n",
        "            file_name = img_info['file_name']\n",
        "            output_path = os.path.join(output_dir, os.path.splitext(file_name)[0] + '.txt')\n",
        "\n",
        "            with open(output_path, 'w') as f:\n",
        "                for ann in annotations:\n",
        "                    category_id = ann['category_id']\n",
        "                    if category_id in category_map: # Check if category_id is in the map\n",
        "                        yolo_class_id = category_map[category_id]\n",
        "                        bbox = ann['bbox']\n",
        "                        # Convert [x, y, width, height] to [center_x, center_y, width, height] (normalized)\n",
        "                        x_center = (bbox[0] + bbox[2] / 2) / img_width\n",
        "                        y_center = (bbox[1] + bbox[3] / 2) / img_height\n",
        "                        bbox_width = bbox[2] / img_width\n",
        "                        bbox_height = bbox[3] / img_height\n",
        "\n",
        "                        f.write(f\"{yolo_class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
        "        # else:\n",
        "            # print(f\"Warning: Image ID {image_id} not found in original COCO image data.\")\n",
        "\n",
        "\n",
        "# Define the mapping from COCO category IDs to YOLO class IDs\n",
        "annotation_file_train = 'annotations/instances_train2014.json'\n",
        "coco_train = COCO(annotation_file_train)\n",
        "categories = coco_train.loadCats(coco_train.getCatIds())\n",
        "category_name_to_id = {cat['name']: cat['id'] for cat in categories}\n",
        "\n",
        "desired_classes = ['cat', 'dog', 'person', 'car', 'bicycle', 'bus']\n",
        "category_map = {}\n",
        "for i, class_name in enumerate(desired_classes):\n",
        "    if class_name in category_name_to_id:\n",
        "        category_map[category_name_to_id[class_name]] = i\n",
        "\n",
        "print(\"Category mapping (COCO ID to YOLO ID):\", category_map)\n",
        "\n",
        "# Convert filtered training annotations using the fixed function\n",
        "coco_to_yolo_fixed('coco_filtered/annotations/filtered_instances_train2014.json', annotation_file_train, 'coco_filtered/labels', category_map)\n",
        "\n",
        "# Convert filtered validation annotations using the fixed function\n",
        "annotation_file_val = 'annotations/instances_val2014.json'\n",
        "coco_to_yolo_fixed('coco_filtered/val_annotations/filtered_instances_val2014.json', annotation_file_val, 'coco_filtered/val_labels', category_map)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8674a72a"
      },
      "source": [
        "import os\n",
        "\n",
        "# 1. Create obj.names file\n",
        "obj_names_path = '/content/darknet/obj.names'\n",
        "desired_classes = ['cat', 'dog', 'person', 'car', 'bicycle', 'bus'] # Ensure this matches the classes chosen in the filtering step\n",
        "\n",
        "with open(obj_names_path, 'w') as f:\n",
        "    for class_name in desired_classes:\n",
        "        f.write(f\"{class_name}\\n\")\n",
        "\n",
        "print(f\"Created {obj_names_path} with classes: {desired_classes}\")\n",
        "\n",
        "# 2. Create or modify obj.data file\n",
        "obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "num_classes = len(desired_classes)\n",
        "\n",
        "obj_data_content = f\"\"\"classes = {num_classes}\n",
        "train = /content/gdrive/My\\\\ Drive/darknet/train.txt\n",
        "valid = /content/gdrive/My\\\\ Drive/darknet/test.txt\n",
        "names = /content/darknet/obj.names\n",
        "backup = /content/gdrive/My\\\\ Drive/darknet/backup/\n",
        "\"\"\"\n",
        "\n",
        "with open(obj_data_path, 'w') as f:\n",
        "    f.write(obj_data_content)\n",
        "\n",
        "print(f\"Created/Modified {obj_data_path}\")\n",
        "\n",
        "# 3. Copy yolov3.cfg to Google Drive\n",
        "original_cfg_path = '/content/darknet/cfg/yolov3.cfg'\n",
        "drive_cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "# Create the darknet directory in My Drive if it doesn't exist\n",
        "os.makedirs(os.path.dirname(drive_cfg_path), exist_ok=True)\n",
        "\n",
        "shutil.copyfile(original_cfg_path, drive_cfg_path)\n",
        "\n",
        "print(f\"Copied {original_cfg_path} to {drive_cfg_path}\")\n",
        "\n",
        "# 4. Modify the copied yolov3.cfg file\n",
        "with open(drive_cfg_path, 'r') as f:\n",
        "    cfg_content = f.readlines()\n",
        "\n",
        "modified_cfg_content = []\n",
        "filters_value = (num_classes + 5) * 3\n",
        "\n",
        "for i, line in enumerate(cfg_content):\n",
        "    # Find [convolutional] layers before [yolo] layers\n",
        "    if line.strip() == '[yolo]':\n",
        "        # Find the preceding [convolutional] layer\n",
        "        for j in range(i - 1, -1, -1):\n",
        "            if cfg_content[j].strip() == '[convolutional]':\n",
        "                # Update filters\n",
        "                k = j + 1\n",
        "                while k < len(cfg_content) and not cfg_content[k].strip().startswith('['):\n",
        "                    if cfg_content[k].strip().startswith('filters='):\n",
        "                        modified_cfg_content.append(f\"filters={filters_value}\\n\")\n",
        "                        print(f\"Updated filters to {filters_value} in convolutional layer before yolo at line {i+1}\")\n",
        "                        break\n",
        "                    k += 1\n",
        "                break\n",
        "        # Update classes in [yolo] layer\n",
        "        modified_cfg_content.append(line)\n",
        "        k = i + 1\n",
        "        while k < len(cfg_content) and not cfg_content[k].strip().startswith('['):\n",
        "            if cfg_content[k].strip().startswith('classes='):\n",
        "                modified_cfg_content.append(f\"classes={num_classes}\\n\")\n",
        "                print(f\"Updated classes to {num_classes} in yolo layer at line {i+1}\")\n",
        "                break\n",
        "            modified_cfg_content.append(cfg_content[k])\n",
        "            k += 1\n",
        "        # Append remaining lines from the original [yolo] block\n",
        "        while k < len(cfg_content) and not cfg_content[k].strip().startswith('['):\n",
        "             modified_cfg_content.append(cfg_content[k])\n",
        "             k += 1\n",
        "    else:\n",
        "        modified_cfg_content.append(line)\n",
        "\n",
        "\n",
        "with open(drive_cfg_path, 'w') as f:\n",
        "    f.writelines(modified_cfg_content)\n",
        "\n",
        "print(f\"Modified {drive_cfg_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgqbxlLqlMMp"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Locate the yolov3.cfg file within the darknet directory\n",
        "darknet_dir = '/content/darknet'\n",
        "cfg_dir = os.path.join(darknet_dir, 'cfg')\n",
        "original_cfg_path = os.path.join(cfg_dir, 'yolov3.cfg')\n",
        "\n",
        "# Verify if the file exists before attempting to copy\n",
        "if not os.path.exists(original_cfg_path):\n",
        "    # If not found in the standard cfg directory, search for it\n",
        "    print(f\"Could not find {original_cfg_path}. Searching for yolov3.cfg in {darknet_dir}...\")\n",
        "    found_cfg_path = None\n",
        "    for root, _, files in os.walk(darknet_dir):\n",
        "        if 'yolov3.cfg' in files:\n",
        "            found_cfg_path = os.path.join(root, 'yolov3.cfg')\n",
        "            break\n",
        "    if found_cfg_path:\n",
        "        original_cfg_path = found_cfg_path\n",
        "        print(f\"Found yolov3.cfg at: {original_cfg_path}\")\n",
        "    else:\n",
        "        print(\"Error: yolov3.cfg not found anywhere in the darknet directory.\")\n",
        "        # If the file is still not found, we cannot proceed with the rest of the steps\n",
        "        # Since we are not allowed to ask for help, we will finish the task with failure.\n",
        "        # The rest of the code block will not be executed if the file is not found.\n",
        "\n",
        "\n",
        "if os.path.exists(original_cfg_path):\n",
        "    # 3. Copy yolov3.cfg to Google Drive\n",
        "    drive_cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "    # Create the darknet directory in My Drive if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(drive_cfg_path), exist_ok=True)\n",
        "\n",
        "    shutil.copyfile(original_cfg_path, drive_cfg_path)\n",
        "\n",
        "    print(f\"Copied {original_cfg_path} to {drive_cfg_path}\")\n",
        "\n",
        "    # 4. Modify the copied yolov3.cfg file\n",
        "    with open(drive_cfg_path, 'r') as f:\n",
        "        cfg_content = f.readlines()\n",
        "\n",
        "    modified_cfg_content = []\n",
        "    num_classes = len(['cat', 'dog', 'person', 'car', 'bicycle', 'bus']) # Ensure this matches the classes chosen in the filtering step\n",
        "    filters_value = (num_classes + 5) * 3\n",
        "\n",
        "    for i, line in enumerate(cfg_content):\n",
        "        # Find [convolutional] layers before [yolo] layers\n",
        "        if line.strip() == '[yolo]':\n",
        "            # Find the preceding [convolutional] layer\n",
        "            for j in range(i - 1, -1, -1):\n",
        "                if cfg_content[j].strip() == '[convolutional]':\n",
        "                    # Update filters\n",
        "                    k = j + 1\n",
        "                    while k < len(cfg_content) and not cfg_content[k].strip().startswith('['):\n",
        "                        if cfg_content[k].strip().startswith('filters='):\n",
        "                            modified_cfg_content.append(f\"filters={filters_value}\\n\")\n",
        "                            print(f\"Updated filters to {filters_value} in convolutional layer before yolo at line {i+1}\")\n",
        "                            break\n",
        "                        k += 1\n",
        "                    break\n",
        "            # Update classes in [yolo] layer\n",
        "            modified_cfg_content.append(line)\n",
        "            k = i + 1\n",
        "            while k < len(cfg_content) and not cfg_content[k].strip().startswith('['):\n",
        "                if cfg_content[k].strip().startswith('classes='):\n",
        "                    modified_cfg_content.append(f\"classes={num_classes}\\n\")\n",
        "                    break\n",
        "                modified_cfg_content.append(cfg_content[k])\n",
        "                k += 1\n",
        "            # Append remaining lines from the original [yolo] block\n",
        "            while k < len(cfg_content) and not cfg_content[k].strip().startswith('['):\n",
        "                 modified_cfg_content.append(cfg_content[k])\n",
        "                 k += 1\n",
        "        else:\n",
        "            modified_cfg_content.append(line)\n",
        "\n",
        "\n",
        "    with open(drive_cfg_path, 'w') as f:\n",
        "        f.writelines(modified_cfg_content)\n",
        "\n",
        "    print(f\"Modified {drive_cfg_path}\")\n",
        "else:\n",
        "    # The file was not found, so the task cannot be completed successfully.\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37a7be8a"
      },
      "source": [
        "import os\n",
        "\n",
        "# Path to the filtered image directories\n",
        "filtered_images_dir = 'coco_filtered/images'\n",
        "filtered_val_images_dir = 'coco_filtered/val_images'\n",
        "\n",
        "# Path to the Darknet directory in Google Drive\n",
        "darknet_drive_dir = '/content/gdrive/My Drive/darknet/'\n",
        "\n",
        "# Create train.txt\n",
        "train_txt_path = os.path.join(darknet_drive_dir, 'train.txt')\n",
        "with open(train_txt_path, 'w') as f:\n",
        "    for img_file in os.listdir(filtered_images_dir):\n",
        "        img_path = os.path.abspath(os.path.join(filtered_images_dir, img_file))\n",
        "        f.write(img_path + '\\n')\n",
        "\n",
        "print(f\"Created train.txt at {train_txt_path} with paths to filtered training images.\")\n",
        "\n",
        "# Create test.txt\n",
        "test_txt_path = os.path.join(darknet_drive_dir, 'test.txt')\n",
        "with open(test_txt_path, 'w') as f:\n",
        "    for img_file in os.listdir(filtered_val_images_dir):\n",
        "        img_path = os.path.abspath(os.path.join(filtered_val_images_dir, img_file))\n",
        "        f.write(img_path + '\\n')\n",
        "\n",
        "print(f\"Created test.txt at {test_txt_path} with paths to filtered validation images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7723e741"
      },
      "source": [
        "# Path to the darknet executable\n",
        "darknet_executable = './darknet'\n",
        "\n",
        "# Paths to the configuration files in Google Drive\n",
        "obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "# Path to the pre-trained weights (using darknet53.conv.74 for transfer learning)\n",
        "# Ensure this file exists in your Google Drive darknet folder\n",
        "weights_path = '/content/gdrive/My Drive/darknet/darknet53.conv.74'\n",
        "\n",
        "# Check if the weights file exists before starting training\n",
        "import os\n",
        "if not os.path.exists(weights_path):\n",
        "    print(f\"Error: Pre-trained weights file not found at {weights_path}\")\n",
        "    print(\"Please ensure darknet53.conv.74 (or your chosen weights) is in your Google Drive darknet folder.\")\n",
        "else:\n",
        "    # Construct the training command\n",
        "    training_command = f\"{darknet_executable} detector train \\\"{obj_data_path}\\\" \\\"{cfg_path}\\\" \\\"{weights_path}\\\" -dont_show\"\n",
        "\n",
        "    # Execute the training command\n",
        "    print(f\"Executing training command: {training_command}\")\n",
        "    !{training_command}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "984d3cdc"
      },
      "source": [
        "# Download the darknet53.conv.74 pre-trained weights\n",
        "!wget https://pjreddie.com/media/files/darknet53.conv.74 -O /content/gdrive/My\\ Drive/darknet/darknet53.conv.74"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69e38845"
      },
      "source": [
        "# Path to the darknet executable\n",
        "darknet_executable = './darknet'\n",
        "\n",
        "# Paths to the configuration files in Google Drive\n",
        "obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "# Path to the pre-trained weights (using darknet53.conv.74 for transfer learning)\n",
        "weights_path = '/content/gdrive/My Drive/darknet/darknet53.conv.74'\n",
        "\n",
        "# Check if the weights file exists before starting training\n",
        "import os\n",
        "if not os.path.exists(weights_path):\n",
        "    print(f\"Error: Pre-trained weights file not found at {weights_path}\")\n",
        "    print(\"Please ensure darknet53.conv.74 (or your chosen weights) is in your Google Drive darknet folder.\")\n",
        "else:\n",
        "    # Construct the training command\n",
        "    training_command = f\"{darknet_executable} detector train \\\"{obj_data_path}\\\" \\\"{cfg_path}\\\" \\\"{weights_path}\\\" -dont_show\"\n",
        "\n",
        "    # Execute the training command\n",
        "    print(f\"Executing training command: {training_command}\")\n",
        "    !{training_command}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7a23b14"
      },
      "source": [
        "import os\n",
        "\n",
        "# Path to the darknet directory\n",
        "darknet_dir = './darknet'\n",
        "\n",
        "# Find the darknet executable within the darknet directory\n",
        "darknet_executable = None\n",
        "for root, _, files in os.walk(darknet_dir):\n",
        "    if 'darknet' in files:\n",
        "        darknet_executable = os.path.join(root, 'darknet')\n",
        "        break\n",
        "\n",
        "if darknet_executable and os.path.isfile(darknet_executable) and os.access(darknet_executable, os.X_OK):\n",
        "    print(f\"Found Darknet executable at: {darknet_executable}\")\n",
        "\n",
        "    # Paths to the configuration files in Google Drive\n",
        "    obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "    cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "    # Path to the pre-trained weights (using darknet53.conv.74 for transfer learning)\n",
        "    weights_path = '/content/gdrive/My Drive/darknet/darknet53.conv.74'\n",
        "\n",
        "    # Check if the weights file exists before starting training\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(f\"Error: Pre-trained weights file not found at {weights_path}\")\n",
        "        print(\"Please ensure darknet53.conv.74 (or your chosen weights) is in your Google Drive darknet folder.\")\n",
        "    else:\n",
        "        # Construct the training command\n",
        "        training_command = f\"{darknet_executable} detector train \\\"{obj_data_path}\\\" \\\"{cfg_path}\\\" \\\"{weights_path}\\\" -dont_show\"\n",
        "\n",
        "        # Execute the training command\n",
        "        print(f\"Executing training command: {training_command}\")\n",
        "        !{training_command}\n",
        "else:\n",
        "    print(\"Error: Darknet executable not found or not executable.\")\n",
        "    # Since the executable was not found, the task cannot be completed successfully.\n",
        "    # We will finish the task with failure.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82930b9a"
      },
      "source": [
        "import os\n",
        "\n",
        "# Path to the darknet directory\n",
        "darknet_dir = './darknet'\n",
        "\n",
        "# Potential paths for the darknet executable based on common build locations\n",
        "potential_exec_paths = [\n",
        "    os.path.join(darknet_dir, 'darknet'),\n",
        "    os.path.join(darknet_dir, 'build', 'darknet', 'x64', 'darknet'),\n",
        "    os.path.join(darknet_dir, 'build', 'darknet', 'darknet'), # Another potential build path\n",
        "    os.path.join(darknet_dir, 'x64', 'darknet') # Another potential build path\n",
        "]\n",
        "\n",
        "darknet_executable = None\n",
        "for exec_path in potential_exec_paths:\n",
        "    if os.path.isfile(exec_path) and os.access(exec_path, os.X_OK):\n",
        "        darknet_executable = exec_path\n",
        "        break\n",
        "\n",
        "if darknet_executable:\n",
        "    print(f\"Found Darknet executable at: {darknet_executable}\")\n",
        "\n",
        "    # Paths to the configuration files in Google Drive\n",
        "    obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "    cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "    # Path to the pre-trained weights (using darknet53.conv.74 for transfer learning)\n",
        "    weights_path = '/content/gdrive/My Drive/darknet/darknet53.conv.74'\n",
        "\n",
        "    # Check if the weights file exists before starting training\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(f\"Error: Pre-trained weights file not found at {weights_path}\")\n",
        "        print(\"Please ensure darknet53.conv.74 (or your chosen weights) is in your Google Drive darknet folder.\")\n",
        "    else:\n",
        "        # Construct the training command\n",
        "        training_command = f\"{darknet_executable} detector train \\\"{obj_data_path}\\\" \\\"{cfg_path}\\\" \\\"{weights_path}\\\" -dont_show\"\n",
        "\n",
        "        # Execute the training command\n",
        "        print(f\"Executing training command: {training_command}\")\n",
        "        !{training_command}\n",
        "else:\n",
        "    print(\"Error: Darknet executable not found or not executable in common build locations.\")\n",
        "    # Since the executable was not found, the task cannot be completed successfully.\n",
        "    # We will finish the task with failure.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe3e5201"
      },
      "source": [
        "# List the contents of the darknet directory and its subdirectories to find the executable\n",
        "!ls -R ./darknet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1735853"
      },
      "source": [
        "import os\n",
        "\n",
        "# Path to the darknet executable found from the listing\n",
        "darknet_executable = './darknet/build/darknet/x64/darknet'\n",
        "\n",
        "# Paths to the configuration files in Google Drive\n",
        "obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "# Path to the pre-trained weights (using darknet53.conv.74 for transfer learning)\n",
        "weights_path = '/content/gdrive/My Drive/darknet/darknet53.conv.74'\n",
        "\n",
        "# Check if the weights file exists before starting training\n",
        "if not os.path.exists(weights_path):\n",
        "    print(f\"Error: Pre-trained weights file not found at {weights_path}\")\n",
        "    print(\"Please ensure darknet53.conv.74 (or your chosen weights) is in your Google Drive darknet folder.\")\n",
        "else:\n",
        "    # Construct the training command\n",
        "    training_command = f\"{darknet_executable} detector train \\\"{obj_data_path}\\\" \\\"{cfg_path}\\\" \\\"{weights_path}\\\" -dont_show\"\n",
        "\n",
        "    # Execute the training command\n",
        "    print(f\"Executing training command: {training_command}\")\n",
        "    !{training_command}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db542f4e"
      },
      "source": [
        "import os\n",
        "\n",
        "# Path to the darknet executable based on observation\n",
        "darknet_executable = './darknet/darknet'\n",
        "\n",
        "# Paths to the configuration files in Google Drive\n",
        "obj_data_path = '/content/gdrive/My Drive/darknet/obj.data'\n",
        "cfg_path = '/content/gdrive/My Drive/darknet/yolov3.cfg'\n",
        "\n",
        "# Path to the pre-trained weights (using darknet53.conv.74 for transfer learning)\n",
        "weights_path = '/content/gdrive/My Drive/darknet/darknet53.conv.74'\n",
        "\n",
        "# Check if the executable exists and is executable\n",
        "if not os.path.isfile(darknet_executable) or not os.access(darknet_executable, os.X_OK):\n",
        "    print(f\"Error: Darknet executable not found or not executable at {darknet_executable}.\")\n",
        "else:\n",
        "    # Check if the weights file exists before starting training\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(f\"Error: Pre-trained weights file not found at {weights_path}\")\n",
        "        print(\"Please ensure darknet53.conv.74 (or your chosen weights) is in your Google Drive darknet folder.\")\n",
        "    else:\n",
        "        # Construct the training command\n",
        "        training_command = f\"{darknet_executable} detector train \\\"{obj_data_path}\\\" \\\"{cfg_path}\\\" \\\"{weights_path}\\\" -dont_show\"\n",
        "\n",
        "        # Execute the training command\n",
        "        print(f\"Executing training command: {training_command}\")\n",
        "        !{training_command}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "892f1108"
      },
      "source": [
        "!ls -la\n",
        "!ls -la ./darknet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c89e0971"
      },
      "source": [
        "import os\n",
        "\n",
        "# Change to the darknet directory\n",
        "os.chdir('./darknet')\n",
        "\n",
        "# Run make to compile darknet\n",
        "!make\n",
        "\n",
        "# Change back to the original directory\n",
        "os.chdir('../')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76eef883"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y libopencv-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b06947aa"
      },
      "source": [
        "import os\n",
        "\n",
        "# Change to the darknet directory\n",
        "os.chdir('./darknet')\n",
        "\n",
        "# Clean previous build artifacts\n",
        "!make clean\n",
        "\n",
        "# Run make again to recompile\n",
        "!make\n",
        "\n",
        "# Change back to the original directory\n",
        "os.chdir('../')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "728fc323"
      },
      "source": [
        "import os\n",
        "\n",
        "# Change to the darknet directory\n",
        "os.chdir('./darknet')\n",
        "\n",
        "# Set PKG_CONFIG_PATH to include the directory containing opencv.pc\n",
        "os.environ['PKG_CONFIG_PATH'] = '/usr/lib/x86_64-linux-gnu/pkgconfig'\n",
        "\n",
        "# Modify the Makefile to enable GPU and CUDNN\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "\n",
        "# Clean previous build artifacts\n",
        "!make clean\n",
        "\n",
        "# Run make again to recompile\n",
        "!make\n",
        "\n",
        "# Change back to the original directory\n",
        "os.chdir('../')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fec30c33"
      },
      "source": [
        "import os\n",
        "\n",
        "# Find the directory containing opencv.pc\n",
        "opencv_pc_dir = !pkg-config --variable pcfiledir opencv\n",
        "opencv_pc_dir = opencv_pc_dir[0].strip()\n",
        "\n",
        "print(f\"Found opencv.pc directory: {opencv_pc_dir}\")\n",
        "\n",
        "# Change to the darknet directory\n",
        "os.chdir('./darknet')\n",
        "\n",
        "# Set PKG_CONFIG_PATH to the found directory\n",
        "os.environ['PKG_CONFIG_PATH'] = opencv_pc_dir\n",
        "\n",
        "# Modify the Makefile to enable GPU and CUDNN (ensure these are set)\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "\n",
        "# Clean previous build artifacts\n",
        "!make clean\n",
        "\n",
        "# Run make again to recompile\n",
        "!make\n",
        "\n",
        "# Change back to the original directory\n",
        "os.chdir('../')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0snm9a2l9av"
      },
      "source": [
        "import os\n",
        "\n",
        "# Search for opencv.pc file in the system\n",
        "find_command = 'find / -name \"opencv.pc\" 2>/dev/null'\n",
        "opencv_pc_path_list = !{find_command}\n",
        "\n",
        "if opencv_pc_path_list:\n",
        "    # Take the first found path and get its directory\n",
        "    opencv_pc_dir = os.path.dirname(opencv_pc_path_list[0])\n",
        "    print(f\"Found opencv.pc directory: {opencv_pc_dir}\")\n",
        "\n",
        "    # Change to the darknet directory\n",
        "    os.chdir('./darknet')\n",
        "\n",
        "    # Set PKG_CONFIG_PATH to the found directory\n",
        "    os.environ['PKG_CONFIG_PATH'] = opencv_pc_dir\n",
        "\n",
        "    # Modify the Makefile to enable GPU and CUDNN (ensure these are set)\n",
        "    !sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "    !sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "\n",
        "    # Clean previous build artifacts\n",
        "    !make clean\n",
        "\n",
        "    # Run make again to recompile\n",
        "    !make\n",
        "\n",
        "    # Change back to the original directory\n",
        "    os.chdir('../')\n",
        "\n",
        "else:\n",
        "    print(\"Error: opencv.pc not found on the system. Cannot compile Darknet with OpenCV support.\")\n",
        "    # Since opencv.pc was not found, the task cannot be completed successfully.\n",
        "    # We will finish the task with failure.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}